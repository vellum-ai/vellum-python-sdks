// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum import (
    ChatMessagePromptBlock,
    PlainTextPromptBlock,
    PromptParameters,
    RichTextPromptBlock,
    VariablePromptBlock,
)
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            text="""\\
Summarize the following text:

\\
"""
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > legacy prompt variant > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > legacy prompt variant > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum import (
    ChatMessagePromptBlock,
    PlainTextPromptBlock,
    PromptParameters,
    RichTextPromptBlock,
    VariablePromptBlock,
)
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            text="""\\
Summarize the following text:

\\
"""
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > reject on error enabled > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


@BaseTryNodeDisplay.wrap(
    node_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
    error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
)
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > reject on error enabled > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum import (
    ChatMessagePromptBlock,
    PlainTextPromptBlock,
    PromptParameters,
    RichTextPromptBlock,
    VariablePromptBlock,
)
from vellum.workflows.nodes.core import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            text="""\\
Summarize the following text:

\\
"""
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum import FunctionDefinition, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function"),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > legacy prompt variant > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > legacy prompt variant > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum import FunctionDefinition, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function"),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > reject on error enabled > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


@BaseTryNodeDisplay.wrap(
    node_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
    error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
)
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > reject on error enabled > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum import FunctionDefinition, PromptParameters
from vellum.workflows.nodes.core import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function"),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeDisplayFile for JINJA block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeFile for JINJA block type 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > JINJA block type > legacy prompt variant > getNodeDisplayFile for JINJA block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > JINJA block type > legacy prompt variant > getNodeFile for JINJA block type 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > JINJA block type > reject on error enabled > getNodeDisplayFile for JINJA block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


@BaseTryNodeDisplay.wrap(
    node_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
    error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
)
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > JINJA block type > reject on error enabled > getNodeFile for JINJA block type 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.core import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum import PlainTextPromptBlock, PromptParameters, RichTextPromptBlock
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(blocks=[PlainTextPromptBlock(text="""Hello World!""")]),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > legacy prompt variant > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > legacy prompt variant > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum import PlainTextPromptBlock, PromptParameters, RichTextPromptBlock
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(blocks=[PlainTextPromptBlock(text="""Hello World!""")]),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > reject on error enabled > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


@BaseTryNodeDisplay.wrap(
    node_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
    error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
)
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > reject on error enabled > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum import PlainTextPromptBlock, PromptParameters, RichTextPromptBlock
from vellum.workflows.nodes.core import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(blocks=[PlainTextPromptBlock(text="""Hello World!""")]),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeDisplayFile for VARIABLE block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeFile for VARIABLE block type 1`] = `
"from vellum import PromptParameters, VariablePromptBlock
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(input_variable="text"),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > legacy prompt variant > getNodeDisplayFile for VARIABLE block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > VARIABLE block type > legacy prompt variant > getNodeFile for VARIABLE block type 1`] = `
"from vellum import PromptParameters, VariablePromptBlock
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(input_variable="text"),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > reject on error enabled > getNodeDisplayFile for VARIABLE block type 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


@BaseTryNodeDisplay.wrap(
    node_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
    error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"),
)
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > VARIABLE block type > reject on error enabled > getNodeFile for VARIABLE block type 1`] = `
"from vellum import PromptParameters, VariablePromptBlock
from vellum.workflows.nodes.core import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(input_variable="text"),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > basic with custom, same module, base > getNodeFile 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters

from .my_inline_prompt import MyInlinePrompt


class PromptNode(MyInlinePrompt):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > basic with node inputs and the prompt_inputs attribute defined > should generate node file prioritizing the latter 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..state import State


class PromptNode(InlinePromptNode[State]):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": State.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > basic with stream setting false > getNodeFile 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters, PromptSettings
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    settings = PromptSettings(stream_enabled=False)
"
`;

exports[`InlinePromptNode > basic with undefined template > should generate node file 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(state="DISABLED", template=""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > should generate cache config correctly 1`] = `
"from vellum import EphemeralPromptCacheConfig, JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(
            cache_config=EphemeralPromptCacheConfig(type="EPHEMERAL"),
            template="""Hello, {{ name }}!""",
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > should generate prompt parameters correctly 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=["foo", "bar"],
        temperature=0.12,
        max_tokens=345,
        top_p=0.67,
        top_k=8,
        frequency_penalty=0.9,
        presence_penalty=0.11,
        logit_bias={
            "foo": 0.1,
            "bar": 0.2,
        },
        custom_parameters={
            "foo": "bar",
        },
    )
"
`;

exports[`InlinePromptNode > with functions attribute > should generate FunctionDefinition objects for CONSTANT_VALUE functions 1`] = `
"from vellum import FunctionDefinition, JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(
            name="parse_text_elements",
            description="Extracts and analyzes key elements from provided text",
            parameters={
                "type": "object",
                "required": [
                    "noun",
                    "verb",
                    "adjective",
                    "summary",
                    "explain",
                ],
                "properties": {
                    "noun": {
                        "type": "string",
                        "description": "A key noun or main subject identified in the text",
                    },
                    "verb": {
                        "type": "string",
                        "description": "A significant action or verb from the text",
                    },
                    "explain": {
                        "type": "string",
                        "description": "A concise explanation of why you chose these specific elements",
                    },
                    "summary": {
                        "type": "string",
                        "description": "A brief summary of the main idea or theme",
                    },
                    "adjective": {
                        "type": "string",
                        "description": "A descriptive adjective that characterizes the text or its subject",
                    },
                },
            },
        ),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > with functions attribute > should generate functions field with WorkflowValueDescriptor for upstream node output 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs
from .my_custom_node import FunctionsProviderNode


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = FunctionsProviderNode.Outputs.functions
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > with json output id defined > getNodeDisplayFile 1`] = `
"from uuid import UUID

from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)

from ...nodes.prompt_node import PromptNode


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    node_input_ids_by_name = {
        "prompt_inputs.text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")
    }
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
        PromptNode.Outputs.json: NodeOutputDisplay(
            id=UUID("some-json-output-id"), name="json"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0))
"
`;

exports[`InlinePromptNode > with ml model node attribute defined > getNodeFile 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > with node trigger AWAIT_ALL > should generate Trigger class with AWAIT_ALL 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.types.core import MergeBehavior

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )

    class Trigger(InlinePromptNode.Trigger):
        merge_behavior = MergeBehavior.AWAIT_ALL
"
`;

exports[`InlinePromptNode > with node trigger AWAIT_ANY > should not generate Trigger class with AWAIT_ANY 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > with trigger attribute input > should generate prompt_inputs with trigger attribute reference 1`] = `
"from vellum import JinjaPromptBlock, PromptParameters
from vellum.workflows.nodes.displayable import InlinePromptNode

from ..triggers.slack_new_message_trigger import SlackNewMessageTrigger


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "channel": SlackNewMessageTrigger.channel_id,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;
