// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`WorkflowProjectGenerator > LazyReference > should not generate LazyReference when there is a long branch 1`] = `
"from vellum.workflows.constants import APIRequestMethod, AuthorizationType
from vellum.workflows.nodes.displayable import APINode as BaseAPINode
from vellum.workflows.references import VellumSecretReference

from .parallel_node_2 import ParallelNode2
from .request_body_node import RequestBodyNode


class APINode(BaseAPINode):
    url = ParallelNode2.Outputs.result
    method = APIRequestMethod.POST
    json = RequestBodyNode.Outputs.result
    api_key_header_key = None
    authorization_type = AuthorizationType.BEARER_TOKEN
    api_key_header_value = None
    bearer_token_value = VellumSecretReference("TEST_SECRET")
"
`;

exports[`WorkflowProjectGenerator > LazyReference > should not generate LazyReference when there is a simple loop 1`] = `
"from vellum.workflows.nodes import BaseNode
from vellum.workflows.ports import Port

from .start_node import StartNode


class RouterNode(BaseNode):
    class Ports(BaseNode.Ports):
        top = Port.on_if(StartNode.Outputs.result.equals("top"))
        bottom = Port.on_else()
"
`;

exports[`WorkflowProjectGenerator > Nodes present but not in graph > should generate unused_graphs if final output is not used 1`] = `
"from vellum.workflows import BaseWorkflow

from .nodes.final_output import FinalOutput
from .nodes.generic_node import GenericNode


class Workflow(BaseWorkflow):
    graph = GenericNode
    unused_graphs = {FinalOutput}

    class Outputs(BaseWorkflow.Outputs):
        final_output = FinalOutput.Outputs.value
"
`;

exports[`WorkflowProjectGenerator > Nodes present but not in graph > should still generate a file for the second node 1`] = `
"from vellum.workflows import BaseWorkflow

from .nodes.first_node import FirstNode
from .nodes.second_node import SecondNode


class Workflow(BaseWorkflow):
    graph = FirstNode
    unused_graphs = {SecondNode}
"
`;

exports[`WorkflowProjectGenerator > Nodes present but not in graph > should still generate a file for the second node 2`] = `
"from vellum.workflows.nodes import BaseNode


class SecondNode(BaseNode):
    class Outputs(BaseNode.Outputs):
        output: str
"
`;

exports[`WorkflowProjectGenerator > Nodes present but not in graph > should still generate a file for the second node 3`] = `
"from vellum_ee.workflows.display.editor import NodeDisplayData, NodeDisplayPosition
from vellum_ee.workflows.display.nodes import BaseNodeDisplay

from ...nodes.second_node import SecondNode


class SecondNodeDisplay(BaseNodeDisplay[SecondNode]):
    display_data = NodeDisplayData(position=NodeDisplayPosition(x=0, y=0), width=None, height=None)
"
`;

exports[`WorkflowProjectGenerator > Nodes present but not in graph > should still generate a file for the second node 4`] = `
"from .first_node import FirstNode
from .second_node import SecondNode

__all__ = [
    "FirstNode",
    "SecondNode",
]
"
`;

exports[`WorkflowProjectGenerator > Nodes present but not in graph > should still generate a file for the second node 5`] = `
"from .first_node import FirstNodeDisplay
from .second_node import SecondNodeDisplay

__all__ = [
    "FirstNodeDisplay",
    "SecondNodeDisplay",
]
"
`;

exports[`WorkflowProjectGenerator > Nodes with forward references > should generate a proper Lazy Reference for the first node 1`] = `
"from vellum.workflows.nodes import BaseNode
from vellum.workflows.references import LazyReference


class FirstNode(BaseNode):
    forward = LazyReference("SecondNode.Outputs.output")
"
`;

exports[`WorkflowProjectGenerator > Try adornment node > should generate try adornment node with correct attributes 1`] = `
"from vellum import PromptParameters
from vellum.workflows.nodes.core.try_node.node import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode


@TryNode.wrap(on_error_code="INVALID_WORKFLOW")
class Prompt(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {}
    parameters = PromptParameters(
        stop=[],
        temperature=None,
        max_tokens=None,
        top_p=None,
        top_k=None,
        frequency_penalty=None,
        presence_penalty=None,
        logit_bias=None,
        custom_parameters=None,
    )
"
`;

exports[`WorkflowProjectGenerator > code execution node at project level > should not autoformat the script file 1`] = `
"import foo, bar
baz = foo + bar
"
`;

exports[`WorkflowProjectGenerator > combinator normalization > should normalize AND to OR combinators 1`] = `
"from vellum.workflows.nodes.displayable import FinalOutputNode
from vellum.workflows.state import BaseState


class FinalOutput(FinalOutputNode[BaseState, str]):
    class Outputs(FinalOutputNode.Outputs):
        value = 3
"
`;

exports[`WorkflowProjectGenerator > failure cases > should generate code even if a node fails to find invalid ports and target nodes 1`] = `
"from vellum.workflows.nodes.displayable import TemplatingNode
from vellum.workflows.state import BaseState


class BadNode(TemplatingNode[BaseState, str]):
    template = """foo"""
    inputs = {}
"
`;

exports[`WorkflowProjectGenerator > failure cases > should generate code even if a node fails to generate 1`] = `
"from vellum.workflows.nodes.displayable import TemplatingNode
from vellum.workflows.state import BaseState


class BadNode(TemplatingNode[BaseState, str]):
    template = """foo"""
    inputs = {}
"
`;

exports[`WorkflowProjectGenerator > include sandbox > should include a sandbox.py file when passed sandboxInputs 1`] = `
"from vellum import ArrayChatMessageContent, ChatMessage, StringChatMessageContent
from vellum.workflows.sandbox import WorkflowSandboxRunner

from .inputs import Inputs
from .workflow import Workflow

if __name__ != "__main__":
    raise Exception("This file is not meant to be imported")


runner = WorkflowSandboxRunner(
    workflow=Workflow(),
    inputs=[
        Inputs(
            input="foo",
            chat=[
                ChatMessage(role="USER", text="foo"),
            ],
        ),
        Inputs(
            input="bar",
            chat=[
                ChatMessage(role="USER", content=StringChatMessageContent(value="bar")),
            ],
        ),
        Inputs(
            input="hello",
            chat=[
                ChatMessage(
                    role="USER",
                    content=ArrayChatMessageContent(
                        value=[
                            StringChatMessageContent(value="hello"),
                        ]
                    ),
                ),
            ],
        ),
    ],
)

runner.run()
"
`;

exports[`WorkflowProjectGenerator > initialization case > should handle workflow with only ENTRYPOINT and TERMINAL nodes 1`] = `
"from vellum.workflows.nodes.displayable import FinalOutputNode
from vellum.workflows.state import BaseState


class FinalOutput(FinalOutputNode[BaseState, str]):
    class Outputs(FinalOutputNode.Outputs):
        pass
"
`;

exports[`WorkflowProjectGenerator > nodes with output values > should prioritize terminal node data over output values 1`] = `
"from vellum.workflows.nodes.displayable import FinalOutputNode
from vellum.workflows.state import BaseState

from .templating_node import TemplatingNode


class FinalOutput(FinalOutputNode[BaseState, str]):
    class Outputs(FinalOutputNode.Outputs):
        value = TemplatingNode.Outputs.result
"
`;

exports[`WorkflowProjectGenerator > retry node adornment > should correctly generate code for a prompt node with retry adornment 1`] = `
"from vellum import (
    ChatMessagePromptBlock,
    PlainTextPromptBlock,
    PromptParameters,
    RichTextPromptBlock,
    VariablePromptBlock,
)
from vellum.workflows.nodes.core.retry_node.node import RetryNode
from vellum.workflows.nodes.displayable import InlinePromptNode


@RetryNode.wrap(max_attempts=5)
class Prompt(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            text="""\\
Summarize the following text: hello today is feb 7 bad weather

\\
"""
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": None,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters=None,
    )
"
`;

exports[`WorkflowProjectGenerator > retry node adornment > should correctly generate default code for adornments 1`] = `
"from vellum import PromptParameters
from vellum.workflows.nodes.core.retry_node.node import RetryNode
from vellum.workflows.nodes.core.try_node.node import TryNode
from vellum.workflows.nodes.displayable import InlinePromptNode


@TryNode.wrap()
@RetryNode.wrap(max_attempts=1)
class Prompt(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {}
    parameters = PromptParameters(
        stop=[],
        temperature=None,
        max_tokens=None,
        top_p=None,
        top_k=None,
        frequency_penalty=None,
        presence_penalty=None,
        logit_bias=None,
        custom_parameters=None,
    )
"
`;

exports[`WorkflowProjectGenerator > runner config with no container image > should handle a runner config with no container image 1`] = `
"from vellum.workflows import BaseWorkflow

from .nodes.templating_node import TemplatingNode


class Workflow(BaseWorkflow):
    graph = TemplatingNode
"
`;

exports[`WorkflowProjectGenerator > should escape special characters > should handle node comments with quotes at the beginning and end 1`] = `
"from vellum.workflows.nodes.displayable import TemplatingNode as BaseTemplatingNode
from vellum.workflows.state import BaseState


class TemplatingNode(BaseTemplatingNode[BaseState, str]):
    """\\"Hello" "World\\""""

    template = """test template"""
    inputs = {}
"
`;

exports[`WorkflowProjectGenerator > should escape special characters > should not escape single quotes in node comments 1`] = `
"from vellum.workflows.nodes.displayable import TemplatingNode as BaseTemplatingNode
from vellum.workflows.state import BaseState


class TemplatingNode(BaseTemplatingNode[BaseState, str]):
    """'Hello' 'World'"""

    template = """test template"""
    inputs = {}
"
`;
